#!/usr/bin/env bash

# This file is part of CO𝘕CEPT, the cosmological 𝘕-body code in Python.
# Copyright © 2015–2020 Jeppe Mosgaard Dakin.
#
# CO𝘕CEPT is free software: You can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# CO𝘕CEPT is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with CO𝘕CEPT. If not, see http://www.gnu.org/licenses/
#
# The author of CO𝘕CEPT can be contacted at dakin(at)phys.au.dk
# The latest version of CO𝘕CEPT is available at
# https://github.com/jmd-dk/concept/



# This script runs the CO𝘕CEPT code.
# Run the script with the -h option to get help.

# Unless this file is being sourced,
# automatically export all variables when set.
being_sourced="True"
if [ "${BASH_SOURCE[0]}" == "${0}" ]; then
    being_sourced="False"
fi
if [ "${being_sourced}" == "False" ]; then
    set -a
fi

# If this file is being sourced, backups of 'this_file' and 'this_dir'
# are needed not to alter the values of these variables.
this_file_backup="${this_file}"
this_dir_backup="${this_dir}"

# Absolute paths to this file and its directory
this_file="$(readlink -f "${BASH_SOURCE[0]}")"
this_dir="$(dirname "${this_file}")"

# The user's current working directory
if [ -z "${workdir}" ]; then
    workdir="$(pwd)"
fi

# For the terminal to be able to print Unicode characters correctly,
# we need to use a UTF-8 locale.
set_locale() {
    # This function will set the locale through the LC_ALL and LANG
    # evironment variables. We want to use a supported UTF-8 locale.
    # The preference order is as follows:
    #   en_US.UTF-8
    #   en_*.UTF-8
    #   C.UTF-8
    #   POSIX.UTF-8
    #   *.UTF-8
    # We consider the suffix (UTF-8) valid regardless of the case and
    # presence of the dash.
    # Get all available locals.
    locales="$(locale -a 2>/dev/null || :)"
    if [ -z "${locales}" ]; then
        return
    fi
    # Look for available UTF-8 locale
    for prefix in "en_US" "en_*" "C" "POSIX" "*"; do
        for suffix in "UTF-8" "UTF8" "utf-8" "utf8"; do
            pattern="${prefix}.${suffix}"
            for loc in ${locales}; do
                if [[ "${loc}" == ${pattern} ]]; then
                    export LC_ALL="${loc}"
                    export LANG="${loc}"
                    return
                fi
            done
        done
    done
}
set_locale
# Set the terminal if unset or broken
if [ -z "${TERM}" ] || [ "${TERM}" == "dumb" ]; then
    export TERM="linux"
fi

# ANSI/VT100 escape sequences
esc="\x1b"
esc_normal="${esc}[0m"
esc_bold="${esc}[1m"
esc_italic="${esc}[3m"
esc_no_italic="${esc}[23m"
esc_red="${esc}[91m"

# Load paths from the .paths file
curr="${this_dir}"
while :; do
    if [ -f "${curr}/.paths" ]; then
        source "${curr}/.paths"
        break
    fi
    if [ "${curr}" == "/" ]; then
        # Print out error message and exit
        printf "${esc_bold}${esc_red}Could not find the .paths file!${esc_normal}\n" >&2
        exit 1
    fi
    curr="$(dirname "${curr}")"
done

# Load environment variables from the .env file
source "${env_file}"

# Add the concept directory to searched paths
# when importing modules in Python.
export PYTHONPATH="${concept_dir}:${PYTHONPATH}"

# Some Python packages may need access to libraries at runtime
for lib in "blas" "gsl" "hdf5" "libpng" "ncurses" "python" "zlib"; do
    eval "lib_dir=\"\${${lib}_dir}\""
    if [ -z "${lib_dir}" ]; then
        continue
    fi
    lib_dir="${lib_dir}/lib"
    if [ ! -d "${lib_dir}" ]; then
        continue
    fi
    export LD_LIBRARY_PATH="${lib_dir}:${LD_LIBRARY_PATH}"
done

# A bug in HDF5 makes the code crash on certain file systems.
# A workaround is to set an environment variable as below.
# The bug is documented here:
# http://www.nersc.gov/users/data-analytics
# /data-management/i-o-libraries/hdf5-2/hdf5/
export HDF5_USE_FILE_LOCKING=FALSE

# The MPI executables and libraries should be
# on the PATH and LD_LIBRARY_PATH, respectively.
export PATH="${mpi_bindir}:${PATH}"
export LD_LIBRARY_PATH="${mpi_libdir}:${LD_LIBRARY_PATH}"
# Additional symlinks to MPI libraries might be placed in mpi_symlinkdir
if [ -d "${mpi_symlinkdir}" ]; then
    export LD_LIBRARY_PATH="${LD_LIBRARY_PATH}:${mpi_symlinkdir}"
    # If the special symlink named "ld_preload.so" is present
    # in mpi_symlinkdir, we should include this symlink in LD_PRELOAD.
    if [ -f "${mpi_symlinkdir}/ld_preload.so" ]; then
        export LD_PRELOAD="${LD_PRELOAD} ${mpi_symlinkdir}/ld_preload.so"
    fi
fi

# The time before any computation begins.
# This time is saved both in seconds after the Unix epoch
# and in a human readable format.
read -r start_time_epoch start_time_human <<<$("${python}" -B -c "
import datetime
t = datetime.datetime.now()
now, ms = str(t).split('.')
print(
    t.timestamp(),
    (now + ms[:3]).replace('-', '').replace(' ', '').replace(':', ''),
)
")

# Check whether this script is run locally or remotely via ssh
ssh="True"
if [ -z "${SSH_CLIENT}" ] && [ -z "${SSH_TTY}" ]; then
    ssh="False"
fi

# Function for printing colored messages
colorprint(){
    # Arguments: Message, color
    "${python}" -B -c "
import sys
from blessings import Terminal
terminal = Terminal(force_styling=True)
print(terminal.bold_${2}('${1}'), file=(sys.stderr if '${2}' == 'red' else sys.stdout))"
}

# Function for printing out a nice CO𝘕CEPT logo
print_logo(){
    logo='
   ____     ____             __  ____    _____   ____   _____
  / __ \   / __ \     /\    / / / __ \  |  ___| |  _ \ |_   _|
 | /  \_| | /  \ |   /  \  / / | /  \_| | |__   | |_) |  | |
 ||    _  ||    ||  / /\ \/ /  ||    _  |  __|  |  __/   | |
 | \__/ | | \__/ | / /  \  /   | \__/ | | |___  | |      | |
  \____/   \____/ /_/    \/     \____/  |_____| |_|      |_|
'
    # Plot the logo via Python's matplotlib.
    # While the color of the 𝘕 is fixed, the color used for the rest
    # is determined from the timestamp of this file.
    # This uses up the the 16th and 17th color of the terminal.
    "${python}" -B -c "
import sys, matplotlib
# Generate color based on input number
color_for_N = 'darkorange'
brightness = lambda color: 0.241*color[0]**2 + 0.691*color[1]**2 + 0.068*color[2]**2
blim = (0.3, 0.97)
color_choices = [
    matplotlib.colors.ColorConverter().to_rgb(color)
    for name, color in
    matplotlib.colors.CSS4_COLORS.items()
    if name != color_for_N
]
color_choices = [color for color in color_choices if blim[0] < brightness(color) < blim[1]]
colors = (
    color_choices[int(sys.argv[1]) % len(color_choices)],
    color_for_N,
)
# Apply colormap
for i, color in enumerate(colors):
    colorhex = matplotlib.colors.rgb2hex(color)
    print('\\x1b]4;{};rgb:{}/{}/{}\\x1b\\\\'
        .format(16 + i, colorhex[1:3], colorhex[3:5], colorhex[5:]), end='')
# Construct the colored logo
logo='''${logo}'''
logo = logo[1:-1]
rows = logo.split('\\n')
ANSI = []
for row in rows:
    ANSI.append('\\x1b[1m')
    for i, c in enumerate(row):
        if i in {0, 18, 31}:
            colornumber = 17 if i == 18 else 16
            ANSI.append(f'\\x1b[38;5;{colornumber}m')
        ANSI.append(c)
    ANSI.append('\\x1b[0m\\n')
# Print the ANSI image
print(''.join(ANSI), end='', flush=True)
" \
    $(stat -c '%Y' "${this_file}")
}
# Print out the logo the first time an execution reaches this point
if [ "${logo_printed}" != "True" ] && [ "${being_sourced}" == "False" ]; then
    print_logo
    export logo_printed="True"
fi

# Function for converting paths to absolute paths
absolute_path(){
    # Arguments: Path, [working directory]
    local path="${1}"
    currdir="$(pwd)"
    if [ -n "${2}" ]; then
        cd "${2}"
    fi
    # Places backslashes before spaces.
    # These are needed when expanding tilde, but they will not persist.
    path="${path//[ ]/\\ }"
    # Expand tilde
    eval path="${path}"
    # Convert to absolute path
    path=$(readlink -m "${path}")
    if [ -z "${path}" ]; then
        colorprint "Cannot convert \"${1}\" to an absolute path!" "red"
        exit 1
    fi
    cd "${currdir}"
    # Print out result
    echo "${path}"
}

# Function for converting an absolute path to its "sensible" form.
# That is, this function returns the relative path with respect to the
# concept directory, if it is no more than one directory above the
# concept directory. Otherwise, return the absolute path back again.
sensible_path(){
    "${python}" -B -c "
path = '${1}'
from os.path import relpath
rel = relpath(path, '${concept_dir}')
print(path if rel.startswith('../../') else rel)"
}

# Function which prints the absolute path of a given command.
# If the command is not an executable file on the PATH but instead a
# known function, the input command is printed as is. If the command
# cannot be found at all, nothing is printed and an exit code of 1
# is returned.
get_command(){
    command_name="${1}"
    # Use the type builtin to locate the command
    local path="$(type "${command_name}" 2>/dev/null | awk '{print $NF}')"
    if [[ "${path}" == "/"* ]]; then
        # The command is a path
        path="$(readlink -f "${path}")"
        echo "${path}"
        return 0
    elif [ -n "${path}" ]; then
        # The command exists as a function
        echo "${command_name}"
        return 0
    fi
    # The command does not exist
    return 1
}

# Function which prints a passed Bash array
# in the format of a Python list.
bash_array2python_list(){
    # Call like this: bash_array2python_list "${array[@]}"
    local list=''
    local element
    for element in "$@"; do
        # If element is a string, encapsulate it in quotation marks
        element=$("${python}" -B -c "
try:
    eval(\"${element}\")
    print(\"${element}\")
except:
    print('\"{}\"'.format(\"${element}\"))
")
        # Append element to list
        list="$(echo "${list}")${element}, "
    done
    list="[$(echo "${list}")]"
    echo "${list}"
}

# Determine the MPI implentation and version
if [ -f "${mpi_bindir}/ompi_info" ] || [ -f "${mpi_compilerdir}/ompi_info" ]; then
    # OpenMPI
    mpi_implementation="openmpi"
    mpi_version="$("${mpiexec}" --version 2>&1 | head -n 1 | awk '{print $NF}')"
elif [ -f "${mpi_bindir}/mpichversion" ] || [ -f "${mpi_compilerdir}/mpichversion" ]; then
    # MPICH or MVAPICH
    for f in "${mpi_bindir}/mpichversion" "${mpi_compilerdir}/mpichversion"; do
        if [ -f "${f}" ]; then
            line="$("${mpi_bindir}/mpichversion" 2>&1 | head -n 1)"
            mpi_implementation="$(echo "${line}" | awk '{print $1}' | tr '[:upper:]' '[:lower:]')"
            if [[ "${mpi_implementation}" == "mvapich"* ]]; then
                mpi_implementation="mvapich"
            else
                mpi_implementation="mpich"
            fi
            mpi_version="$(echo "${line}" | awk '{print $NF}')"
            break
        fi
    done
else
    # Unknown MPI implementation
    mpi_implementation="unknown"
    mpi_version="unknown"
fi

# Function which prints the resource manager.
# In order, the implemented resource managers are:
# - Slurm
# - TORQUE/PBS
get_resource_manager(){
    # Detect what resource manager is used
    if get_command sbatch >/dev/null; then
        # Slurm is installed. Use this as the resource manager.
        resource_manager="slurm"
    elif get_command qsub >/dev/null; then
        # TORQUE/PBS is installed. Use this as the resource manager.
        resource_manager="torque"
    else
        # No resource manager found
        resource_manager=""
    fi
    echo "${resource_manager}"
}

# If this file is being sourced, return now
if [ "${being_sourced}" == "True" ]; then
    this_file="${this_file_backup}"
    this_dir="${this_dir_backup}"
    return
fi

# Set up error trapping
ctrl_c(){
    trap : 0
    exit 2
}
abort(){
    exit_code_newest=$?
    colorprint "An error occurred!" "red"
    if [ -n "${exit_code}" ]; then
        exit ${exit_code}
    elif [ ${exit_code_newest} -ne 0 ]; then
        exit ${exit_code_newest}
    else
        exit 1
    fi
}
trap 'ctrl_c' SIGINT
trap 'abort' EXIT
set -e

# Default values of command-line arguments
interactive_default="False"
local_default="False"
main_default="${concept_dir}/main.py"
memory_default=0  # 0 implies unset
native_optimizations_default="False"
no_optimizations_default="False"
no_recompilation_default="False"
no_watching_default="False"
nprocs_default=1
params_default="None"
pure_python_default="False"
unsafe_building_default="False"
walltime_default="00:00:00"  # 00:00:00 implies unset

# Initial but illegal values of some command-line arguments,
# for testing whether these arguments have been supplied.
nprocs_unspecified="-1"
params_unspecified="__none__"
queue_unspecified="__none__"
test_unspecified="__none__"
utility_unspecified="__none__"

# Change to the concept code directory
cd "${concept_dir}"

# Use Python's argparse module to handle command-line arguments
argparse_finished="no"
argparse_exit_code=""
args=$("${python}" -B -c "
import argparse, math, re, sys
# Function which checks whether input is a representation of
# a positive integer and converts it.
def positive_int(value):
    value_raw = value
    def raise_argparse_exception():
        raise argparse.ArgumentTypeError(\"invalid positive int value: '{}'\".format(value_raw))
    try:
        value = float(eval(value))
    except:
        raise_argparse_exception()
    if value != int(value):
        raise_argparse_exception()
    value = int(value)
    if value < 1:
        raise_argparse_exception()
    return value
# Function which checks whether input is a representation of
# one or two positive integers. If two ints are given,
# separate them by a colon.
def positive_int_or_int_pair(value, value_input=None):
    if value_input is None:
        value_input = value
    def raise_argparse_exception():
        raise argparse.ArgumentTypeError(
            f\"invalid positive int or int pair: '{value_input}'\"
        )
    for sep in ',;':
        value = value.replace(sep, ':')
    if value.count(':') > 1:
        raise_argparse_exception()
    elif value.count(':') == 1:
        values = value.split(':')
        return ':'.join(positive_int_or_int_pair(value, value_input) for value in values)
    else:  # value.count(':') == 0
        try:
            value_eval = eval(value)
        except:
            return positive_int_or_int_pair(value.replace(' ', ':'), value_input)
        try:
            value_float = float(value_eval)
        except:
            raise_argparse_exception()
        value_int = int(value_float)
        if value_int == value_float and value_int > 0:
            return str(value_int)
        raise_argparse_exception()
# Function which checks whether input is a representation of
# a memory size and converts it to bytes.
def memory(value):
    value_raw = value
    def raise_argparse_exception():
        raise argparse.ArgumentTypeError(\"invalid memory value: '{}'\".format(value_raw))
    # Convert to (whole) bytes
    value = value.lower()
    value = value.replace(' ', '').replace('b', '')
    value = re.subn('([0-9]+)([a-z]+)', '\g<1>*\g<2>', value)[0]
    units = {'k': 2**10,
             'm': 2**20,
             'g': 2**30,
             't': 2**40,
             'p': 2**50,
             'e': 2**60,
             'z': 2**70,
             'y': 2**80,
             }
    try:
        value = int(math.ceil(float(eval(value, units))))
    except:
        raise_argparse_exception()
    return value
# Function which converts a time value to the format hh:mm:ss
def time(value):
    # Convert value to integer seconds
    units = {'s': 1}
    units['sec'] = units['secs'] = units['seond'] = units['seonds'] = units['s']
    units['m'] = 60*units['s']
    units['min'] = units['mins'] = units['minute'] = units['minutes'] = units['m']
    units['h'] = 60*units['m']
    units['hr'] = units['hrs'] = units['hs'] = units['hour'] = units['hours'] = units['h']
    units['d'] = 24*units['h']
    units['day'] = units['days'] = units['d']
    units['y'] = 365.25*units['d']
    units['yr'] = units['year'] = units['years'] = units['y']
    # If a pure number is provided, interpret this in seconds.
    try:
        value = int(value)*units['s']
    except:
        pass
    # Attempt to interpret the time as an expression
    # like '2hr + 30mins'.
    if isinstance(value, str):
        value = value.lower()
        value = re.subn('([0-9]+)([a-z]+)', '\g<1>*\g<2>', value)[0]
        try:
            value = int(math.ceil(float(eval(value, units))))
        except:
            pass
    # Attempt to interpret the time in the format
    # 'm:s' or 'h:m:s' or 'd:h:m:s' or 'd+h:m:s' or 'd+h:m' or 'd+h'.
    if isinstance(value, str):
        plusses_in_value = value.count('+')
        if plusses_in_value > 1:
            raise argparse.ArgumentTypeError(\"error parsing value\")
        for sep in '+ ,;':
            value = value.replace(sep, ':')
        value = value.split(':')
        s = m = h = d = 0
        if len(value) == 1:
            raise argparse.ArgumentTypeError(\"error parsing value\")
        elif len(value) == 2:
            if plusses_in_value:
                # Format d:h
                d, h = value
            else:
                # Format m:s
                m, s = value
        elif len(value) == 3:
            if plusses_in_value:
                # Format d+h:m
                d, h, m = value
            else:
                # Format h:m:s
                h, m, s = value
        elif len(value) == 4:
            # Format d:h:m:s or d+h:m:s
            d, h, m, s = value
        else:
            raise argparse.ArgumentTypeError(\"error parsing value\")
        d, h, m, s = int(d), int(h), int(m), int(s)
        value = int(math.ceil(s*units['s'] + m*units['m'] + h*units['h'] + d*units['d']))
    # Now value should be in integer seconds
    if value < 0:
        raise argparse.ArgumentTypeError(\"the wall time cannot be negative\")
    # Convert to the format hh:mm:ss.
    h = value//units['h']
    value -= h*units['h']
    m = value//units['m']
    value -= m*units['m']
    s = value
    h, m, s = str(h), str(m), str(s)
    if len(h) == 1:
        h = '0' + h
    if len(m) == 1:
        m = '0' + m
    if len(s) == 1:
        s = '0' + s
    value = f'{h}:{m}:{s}'
    return value
# Setup command-line arguments
parser = argparse.ArgumentParser(
    prog='$(basename "${this_file}")',
    description='Run the CO𝘕CEPT code',
)
parser.add_argument(
    '-c', '--command-line-params',
    help=(
        'specify parameter(s) directly from the command-line. '
        'If a parameter file is specified as well, the command-line '
        'parameters will take precedence. This option may be specified '
        'multiple times.'
    ),
    default=[],
    action='append',
)
parser.add_argument(
    '-i', '--interactive',
    help='inspect interactively after program execution',
    default=${interactive_default},
    action='store_true',
)
parser.add_argument(
    '-j', '--job-directive',
    help=(
        'specify an additional line to add to the jobscript header '
        'for remote jobs. This option may be specified multiple times.'
    ),
    default=[],
    action='append',
)
parser.add_argument(
    '-m', '--main',
    help='entry point of the code. Can be a Python filename or command.',
    default='${main_default}',
)
parser.add_argument(
    '--memory',
    help='maximum total memory consumption for remote job',
    type=memory,
    default=${memory_default},
)
parser.add_argument(
    '-n', '--nprocs',
    help=('total number of processes '
          'or number of nodes and number of processes per node'),
    type=positive_int_or_int_pair,
    default=${nprocs_unspecified},
)
parser.add_argument(
    '-p', '--params',
    help='parameter file to use',
    default='${params_unspecified}',
)
parser.add_argument(
    '-q', '--queue',
    help='queue for submission of the remote job',
    default='${queue_unspecified}',
)
parser.add_argument(
    '-t', '--test',
    help=('run test TEST. TEST can be any subdirectory of the tests directory. '
          'Use TEST=all to run all tests'),
    default='${test_unspecified}',
)
parser.add_argument(
    '-u', '--utility',
    nargs='+',
    help='run utility UTILITY. UTILITY can be any executable in the utilities directory',
    default=['${utility_unspecified}']*2,  # One for utility, one for utility_args
)
parser.add_argument(
    '-w', '--walltime',
    help='wall time for remote job',
    type=time,
    default='${walltime_default}',
)
parser.add_argument(
    '--local',
    help='force the run to be done locally, without submitting it as a remote job',
    default=${local_default},
    action='store_true',
)
parser.add_argument(
    '--native-optimizations',
    help='allow the compiler to generate non-portable code optimized for this machine',
    default=${native_optimizations_default},
    action='store_true',
)
parser.add_argument(
    '--no-optimizations',
    help='disable compiler optimizations',
    default=${no_optimizations_default},
    action='store_true',
)
parser.add_argument(
    '--no-recompilation',
    help='do not re-cythonize and recompile before running',
    default=${no_recompilation_default},
    action='store_true',
)
parser.add_argument(
    '--no-watching',
    help='do not follow the submitted job via the watch utility',
    default=${no_watching_default},
    action='store_true',
)
parser.add_argument(
    '--pure-python',
    help='run in pure Python mode',
    default=${pure_python_default},
    action='store_true',
)
parser.add_argument(
    '--unsafe-building',
    help='ignore dependencies between modules when building',
    default=${unsafe_building_default},
    action='store_true',
)
parser.add_argument(
    '-v', '--version',
    help='print version info',
    default=False,
    action='store_true',
)
# Enables Python to write directly to screen (stderr)
# in case of help request.
stdout_copy = sys.stdout
sys.stdout = sys.stderr
# Now do the actual argument parsing,
# including writing out the help message.
args, unknown_args = parser.parse_known_args()
# If a utility is to be used, arguments unknown to this script should
# be passed on to the utility.
utility_args = args.utility[1:]
if args.utility[0] != '${utility_unspecified}':
    utility_args += unknown_args
else:
    # Do print out error if invalid arguments are given
    # and no utility should be used.
    args = parser.parse_args()
# Function for converting a list to a str in the form of a Bash array
def python_list2bash_array(lst):
    return '({})'.format(\"'\" + \"' '\".join(
        [str(val).replace(\"'\", '\"') for val in lst]
    ) + \"'\")
# Reset stdout
sys.stdout = stdout_copy
# Print out the arguments.
# These will be captured in the Bash 'args' variable
print('; '.join([
    \"argparse_finished=yes\",
    \"command_line_params='{}'\".format(' ; '.join(args.command_line_params).replace(\"'\", '\"')),
    \"interactive={}\".format(args.interactive),
    \"job_directive='{}'\".format(' ; '.join(args.job_directive).replace(\"'\", '\"')),
    \"main='{}'\".format(str(args.main).replace(\"'\", '\"')),
    \"memory='{}'\".format(args.memory),
    \"nprocs={}\".format(args.nprocs),
    \"params='{}'\".format(args.params),
    \"queue='{}'\".format(args.queue),
    \"test='{}'\".format(args.test),
    \"utility='{}'\".format(args.utility[0]),
    \"utility_args={}\".format(python_list2bash_array(utility_args)),
    \"walltime={}\".format(args.walltime),
    \"local={}\".format(args.local),
    \"native_optimizations={}\".format(args.native_optimizations),
    \"no_optimizations={}\".format(args.no_optimizations),
    \"no_recompilation={}\".format(args.no_recompilation),
    \"no_watching={}\".format(args.no_watching),
    \"pure_python={}\".format(args.pure_python),
    \"unsafe_building={}\".format(args.unsafe_building),
    \"version={}\".format(args.version),
]))
" "$@" || echo "argparse_exit_code=$?")
# Evaluate the handled arguments into this scope
eval "${args}"

# Exit if argparse exited without finishing
if [ "${argparse_finished}" != "yes" ]; then
    if [ -z "${argparse_exit_code}" ]; then
        argparse_exit_code=0
    fi
    if [ ${argparse_exit_code} -eq 0 ]; then
        trap : 0
    fi
    exit ${argparse_exit_code}
fi

# Print version info and exit, if requested
if [ "${version}" == "True" ]; then
    "${concept}" -m 'import commons; print(commons.__version__)' \
        --local --pure-python | tail -n 1
    trap : 0
    exit 0
fi

# Display warning if the requested memory is below one megabyte
if [ ${memory} -gt 0 ] && [ ${memory} -lt 1048576 ]; then
    colorprint "Warning: The requested memory is below 1 MB. \
Have you forgotten to specify the unit?" "red"
fi

# Display warning if the requested wall time is below one minute
if [ "${walltime}" != "00:00:00" ] && [[ "${walltime}" == "00:00:"* ]]; then
    colorprint "Warning: The requested wall time is below 1 minute. \
Have you forgotten to specify the unit?" "red"
fi

# Check whether the main "file" is really a string of commands
main_as_command="no"
if (   [[ "${main}" == *"print("* ]] \
    || [[ "${main}" == *";"*      ]] \
    || [[ "${main}" == *$'\n'*    ]]); then
    main_as_command="yes"
fi

# Convert all supplied paths to absolute paths
if [ "${main_as_command}" == "no" ]; then
    main="$(absolute_path "${main}")"
fi
if [ "${params}" != "${params_unspecified}" ]; then
    params="$(absolute_path "${params}")"
fi
if [ "${test}" != "${test_unspecified}" ] && [ "${test}" != "all" ]; then
    test="${tests_dir}/$(basename "${test}")"
fi
if [ "${utility}" != "${utility_unspecified}" ]; then
    utility="${utilities_dir}/$(basename "${utility}")"
fi

# Function for doing fuzzy comparisons between
# illegal concept options and possible correct ones.
concept_options=("$@")
suggest_correct_invocation(){
    # First argument: The illegal option
    # Second argument: The option type ('test' or 'utility')
    illegal_option="$(basename "$1")"
    invocation="$0"
    for arg in "${concept_options[@]}"; do
        if [ "${replace_next}" == "True" ]; then
            invocation="${invocation} __replace__"
            replace_next="False"
        else
            invocation="${invocation} ${arg}"
        fi
        if    (   [ "$2" == "test"    ] \
               && ([ "${arg}" == "-t" ] || [ "${arg}" == "--test" ])) \
           || (   [ "$2" == "utility" ] \
               && ([ "${arg}" == "-u" ] || [ "${arg}" == "--utility" ])); then
            replace_next="True"
        fi
    done
    "${python}" -B -c "
import difflib, shutil, os
if '$2' == 'test':
    files = os.listdir('${tests_dir}')
    possibilities = [file for file in files if os.path.isdir('${tests_dir}/' + file)]
elif '$2' == 'utility':
    files = os.listdir('${utilities_dir}')
    possibilities = [file for file in files if shutil.which('${utilities_dir}/' + file)]
max_ratio = 0
for possibility in possibilities:
    ratio = max([
        difflib.SequenceMatcher(a='${illegal_option}', b=possibility).ratio(),
        difflib.SequenceMatcher(a='${illegal_option}'.lower(), b=possibility.lower()).ratio(),
    ])
    if ratio > max_ratio:
        max_ratio = ratio
        closest_match = possibility
if max_ratio > 0.01:
    print('Did you mean:\n${invocation}'.replace('__replace__', closest_match))
                   "
}

# Do the supplied paths exist?
if [ "${main_as_command}" == "no" ] && [ ! -f "${main}" ]; then
    colorprint "Error: Entry point \"${main}\" does not exist!" "red"
    exit 1
fi
if [ "${params}" != "${params_unspecified}" ] && [ ! -f "${params}" ]; then
    colorprint "Error: Parameter file \"${params}\" does not exist!" "red"
    exit 1
fi
if [ "${test}" != "${test_unspecified}" ] && [ "${test}" != "all" ] && [ ! -d "${test}" ]; then
    colorprint "Error: Test \"${test}\" does not exist!" "red"
    # Suggest closest match
    suggest_correct_invocation "${test}" "test"
    exit 1
fi
if [ "${utility}" != "${utility_unspecified}" ] && [ ! -f "${utility}" ]; then
    colorprint "Error: Utility \"${utility}\" does not exist!" "red"
    # Suggest closest match
    suggest_correct_invocation "${utility}" "utility"
    exit 1
fi

# Assigned values to unspecified parameters
if [ "${nprocs}" == "${nprocs_unspecified}" ]; then
    nprocs="${nprocs_default}"
fi
if [ "${params}" == "${params_unspecified}" ]; then
    params="${params_default}"
fi

# Check for syntax errors in supplied parameter file.
# Other types of errors will not be detected before runtime.
if [ "${params}" != "${params_default}" ]; then
    params_traceback="$("${python}" -B -c "
import ast
with open('${params}', 'r', encoding='utf-8') as f:
    source = f.read()
ast.parse(source)
" 2>&1 || :)"
    if [ -n "${params_traceback}" ]; then
        colorprint "Syntax error in parameter file \"${params}\":" "red"
        echo "${params_traceback}"
        exit 1
    fi
fi
# Check for syntax errors in supplied command-line parameters.
# Other types of errors will not be detected before runtime.
if [ -n "${command_line_params}" ]; then
    command_line_params_traceback="$("${python}" -B -c "
import ast
source = '${command_line_params}'
ast.parse(source)
" 2>&1 || :)"
    if [ -n "${command_line_params_traceback}" ]; then
        if [[ "${command_line_params_traceback}" == *";"* ]]; then
            colorprint "Syntax error in command-line parameters:" "red"
        else
            colorprint "Syntax error in command-line parameter:" "red"
        fi
        echo "${command_line_params_traceback}"
        exit 1
    fi
fi

# Determine wheter to run CO𝘕CEPT locally or remotely (via some
# resource manager). Always treat tests as if they were run locally.
remote="False"
if     [ "${local}" == "False"               ] \
    && [ "${test}"  == "${test_unspecified}" ] \
    && [ "${ssh}"   == "True"                ]; then
    remote="True"
fi

# MPI implementation specifics
mpiexec_args=""
mpi_env=""
if [ "${mpi_implementation}" == "openmpi" ]; then
    # Disable aggregation of OpenMPI warnings
    mpi_env="${mpi_env}
export OMPI_MCA_orte_base_help_aggregate=0"
    # Disable OpenMPI warning about forking
    mpi_env="${mpi_env}
export OMPI_MCA_mpi_warn_on_fork=0"
    # By default, OpenMPI disallows any usage by the root user
    mpi_env="${mpi_env}
export OMPI_ALLOW_RUN_AS_ROOT=1
export OMPI_ALLOW_RUN_AS_ROOT_CONFIRM=1"
    # In OpenMPI 3 and 4, oversubscription (having more MPI processes
    # than physical cores) is disallowed by default.
    mpi_version_major="${mpi_version:0:1}"
    if [ "${mpi_version_major}" -ge 3 2>/dev/null ]; then
        mpi_env="${mpi_env}
export OMPI_MCA_rmaps_base_oversubscribe=1"
    fi
    # In OpenMPI 4, infiniband ports are disabled by default in favour
    # of UCX. If this leads to an error about not initializing an
    # OpenFabrics device, we overwrite this default.
    if [ -z "${OMPI_MCA_btl_openib_allow_ib}" ] \
        && [ "${mpi_version_major}" -ge 4 2>/dev/null ]; then
        mpi_warning="$("${python}" -B -c '
import mpi4py.rc; mpi4py.rc.threads = False  # Do not use threads
from mpi4py import MPI
' 2>&1)"
        if [ -n "${mpi_warning}" ]; then
            if echo "${mpi_warning}" | grep "error" | grep "OpenFabrics" >/dev/null; then
                if echo "${mpi_warning}" | grep "btl_openib_allow_ib" >/dev/null; then
                    mpi_env="${mpi_env}
export OMPI_MCA_btl_openib_allow_ib=1"
                fi
            fi
        fi
    fi
    # Disable automatic process binding/affinity, allowing OpenMP
    # threads to be assigned to cores in a one-to-one fashion.
    # This is off by default prior to OpenMPI version 1.7.
    # All of 1.7 -- 4.0 supports the --bind-to none option to mpiexec.
    # Here we add this option if it is understood.
    mpiexec_output="$("${mpiexec}" --bind-to none -n 1 echo "success" 2>&1 || :)"
    if [ "${mpiexec_output}" == "success" ]; then
        mpiexec_args="${mpiexec_args} --bind-to none"
    fi
    # For OpenMPI 1.8 -- 4.0 process binding can also be deactivated
    # using the following environment variable.
    mpi_env="${mpi_env}
export OMPI_MCA_hwloc_base_binding_policy=none"
elif [ "${mpi_implementation}" == "mpich" ]; then
    # Though MPICH should not use process binding by default,
    # it still supports a -bind-to none or --bind-to none option.
    # Here we check if this is so, and apply it.
    mpiexec_output="$("${mpiexec}" -bind-to none -n 1 echo "success" 2>&1 || :)"
    if [ "${mpiexec_output}" == "success" ]; then
        mpiexec_args="${mpiexec_args} -bind-to none"
    else
        mpiexec_output="$("${mpiexec}" --bind-to none -n 1 echo "success" 2>&1 || :)"
        if [ "${mpiexec_output}" == "success" ]; then
            mpiexec_args="${mpiexec_args} --bind-to none"
        fi
    fi
elif [ "${mpi_implementation}" == "mvapich" ]; then
    # Disable automatic process binding, allowing OpenMP threads to be
    # assigned to cores in a one-to-one fashion.
    mpi_env="${mpi_env}
export MV2_ENABLE_AFFINITY=0"
fi
if [ -n "${mpi_env}" ]; then
    eval "${mpi_env}"
    mpi_env="# Environment variables${mpi_env}"
fi

# If a test is to be run, run it and exit
if [ "${test}" != "${test_unspecified}" ]; then
    # Function which can extract parameters from the parameter file
    # given by "${this_dir}/params". This is used by some tests.
    get_param(){
        "${concept}"                                                \
            -c="${command_line_params}"                             \
            -m="
from commons import *
try:
    print('param_var =', ${1})
except NameError:
    print('param_var =', user_params['''${1}'''])
"                                                                   \
            -n=1                                                    \
            -p="${this_dir}/params"                                 \
            --local                                                 \
            --pure-python                                           \
        | grep "param_var"                                          \
        | tail -n 1                                                 \
        | sed 's/^.\{12\}//'
    }
    if [ "${test}" == "all" ]; then
        # Find and run all tests
        tests="$(cd "${tests_dir}" && "${python}" -B -c "
from glob import glob
# List tests in order of required execution.
# Tests not included here will be run last.
order = (
    # Test whether the code is able to compile and run
    'basic',
    # Tests of the CLASS installation,
    # the Friedmann equation, realizations
    # and the power spectrum functionality.
    'friedmann',
    'realize',
    'powerspec',
    # Test of the GADGET installation
    'gadget',
    # Tests of the particle implementation
    'drift_nohubble',
    'drift',
    'kick_pp_without_ewald',
    'kick_pp_with_ewald',
    # Tests of the PP implementation
    'nprocs_pp',
    'pure_python_pp',
    'concept_vs_gadget_pp',
    # Tests of the PM implementation
    'nprocs_pm',
    'pure_python_pm',
    'concept_vs_class_pm',
    # Tests of the P³M implementation
    'nprocs_p3m',
    'pure_python_p3m',
    'concept_vs_gadget_p3m',
    # Test multi(particle)component simulations
    'multicomponent',
    # Tests of the fluid implementation
    'fluid_drift_rigid_nohubble',
    'fluid_drift_rigid',
    'fluid_gravity_nohubble',
    'fluid_gravity',
    'fluid_vacuum',
    'fluid_vs_particles',
    'fluid_pressure',
    # To test the fluid implementation on a real life scenario,
    # we test linear and non-linear neutrino simulations.
    'neutrino',
    # Test whether the optimizations introduces bugs
    'optimizations',
    # Tests of other functionality
    'render',
)
# Find all tests (directories in ${tests_dir}).
# Skip test if its (directory) name has a leading underscore.
tests = (dir[:-1] for dir in glob('*/') if not dir.startswith('_'))
# Sort the tests based on the order given above
sorted_tests = sorted(tests, key=lambda test: order.index(test) if test in order else len(order))
for test in sorted_tests:
    print(test)
"              )"
        # Run all tests in the test directory
        echo "The following tests will be run in order:"
        for test_name in ${tests}; do
            echo "    ${test_name}"
        done
        for test_name in ${tests}; do
            start_time_test=$("${python}" -B -c "import time; print(time.time())")
            colorprint "\nRunning ${test_name} test" "yellow"
            "${tests_dir}/${test_name}/run_test"
            colorprint "${test_name} test ran successfully" "green"
            # Print out the execution time for this test
            "${python}" -B -c "from commons import *
print('Total execution time: {}'.format(time_since(${start_time_test})))"
        done
        echo
        colorprint "All tests ran successfully" "green"
    else
        # Run specific test
        test_name="$(basename "${test}")"
        colorprint "Running ${test_name} test" "yellow"
        "${test}/run_test"
        colorprint "${test_name} test ran successfully" "green"
    fi
    # Print out the total time it took to ran the test(s)
    "${python}" -B -c "from commons import *
print('Total execution time: {}'.format(time_since(${start_time_epoch})))"
    # Deactivate traps and exit
    trap : 0
    exit 0
fi

# Cannot run in interactive mode when running remotely
if [ "${remote}" == "True" ] && [ "${interactive}" == "True" ]; then
    colorprint "Cannot run in interactive mode when run remotely" "red"
    exit 1
fi

# Compile or do cleanup from last compilation.
# The only time where neither should be done
# is when running some particular utilities.
prepare_files="True"
if    [ "$(basename "${utility}")" == "play"   ] \
   || [ "$(basename "${utility}")" == "update" ] \
   || [ "$(basename "${utility}")" == "watch"  ]; then
   prepare_files="False"
fi
if [ "${prepare_files}" == "True" ]; then
    if [ "${pure_python}" == "True" ]; then
        # Rename compiled Cython modules *.so to *.so_
        if ls "${concept_dir}/"*.so > /dev/null 2>&1; then
            (cd "${concept_dir}" && for f in *.so; do
                                        mv "${f}" "${f%.so}.so_"
                                    done
             )
        fi
    else
        # Rename compiled Cython modules *.so_ back to *.so
        if ls "${concept_dir}/"*.so_ > /dev/null 2>&1; then
            (cd "${concept_dir}" && for f in *.so_; do
                                        mv "${f}" "${f%.so_}.so"
                                    done
             )
        fi
        if [ "${no_recompilation}" != "True" ]; then
            # When running locally, set the "make_jobs" variable,
            # holding the -j option for future make commands, enabling
            # parallel building. Some implementations of make does not
            # support the bare -j option without explicitly specifying a
            # number afterwards. If so, we do not make use of
            # the -j option.
            if [ "${ssh}" == "False" ] && [ -z "${make_jobs}" ]; then
                tmp_dir_preexists="False"
                if [ -d "${top_dir}/tmp" ]; then
                    tmp_dir_preexists="True"
                fi
                make_jobs_test_dir="${top_dir}/tmp/make_jobs_test"
                rm -rf "${make_jobs_test_dir}" 2>/dev/null || :
                mkdir -p "${make_jobs_test_dir}"
                printf "
test:
\t@echo success
" > "${make_jobs_test_dir}/Makefile"
                make_jobs_output="$(cd "${make_jobs_test_dir}" && make -j 2>/dev/null)" || :
                if [ "${make_jobs_output}" == "success" ]; then
                    make_jobs="-j"
                fi
                if [ "${tmp_dir_preexists}" == "True" ]; then
                    rm -rf "${make_jobs_test_dir}" 2>/dev/null || :
                else
                    rm -rf "${top_dir}/tmp" 2>/dev/null || :
                fi
            fi
            # Compile all the modules in parallel.
            # Cython might warn about redeclarations in CythonGSL,
            # which we filter out as we do not want to worry about them.
            start_time_build=$("${python}" -B -c "import time; print(time.time())")
            exec 4>&1
            make_output="$(
                cd "${concept_dir}" && make                        \
                    native_optimizations="${native_optimizations}" \
                    no_optimizations="${no_optimizations}"         \
                    unsafe_building="${unsafe_building}"           \
                    ${make_jobs}                                   \
                    2> >(grep -v 'GSL_.* redeclared' 1>&2)         \
                    | tee >(cat >&4)                               \
                    ; echo "concept_exit_code = ${PIPESTATUS[0]}"  \
            )"
            exec 4>&-
            exit_code="$(echo "${make_output}" | grep "concept_exit_code" \
                | awk '{print $NF}' || :)"
            if [ "${exit_code}" != "0" ]; then
                colorprint "Failed to compile CO𝘕CEPT!" "red"
                exit 1
            fi
            if [[ "${make_output}" == *"Building modules"* ]]; then
                "${python}" -B -c "from commons import *
print('Build time: {}'.format(time_since(${start_time_build})))"
            fi
        fi
    fi
fi

# If a utility is to be run, run it and exit
if [ "${utility}" != "${utility_unspecified}" ]; then
    # If no argument was passed after the -u option,
    # utility_args should be empty.
    if [ "${utility_args}" == '""' ] || [ "${utility_args}" == "''" ]; then
        utility_args=""
    fi
    # Set flag variables for the flag command options,
    # so that a utility can call this script with the same flags enabled
    # as was used to invoke this script originally.
    interactive_flag=""
    if [ "${interactive}" == "True" ]; then
        interactive_flag="--interactive"
    fi
    local_flag=""
    if [ "${local}" == "True" ]; then
        local_flag="--local"
    fi
    native_optimizations_flag=""
    if [ "${native_optimizations}" == "True" ]; then
        native_optimizations_flag="--native-optimizations"
    fi
    no_optimizations_flag=""
    if [ "${no_optimizations}" == "True" ]; then
        no_optimizations_flag="--no-optimizations"
    fi
    no_recompilation_flag=""
    if [ "${no_recompilation}" == "True" ]; then
        no_recompilation_flag="--no-recompilation"
    fi
    no_watching_flag=""
    if [ "${no_watching}" == "True" ]; then
        no_watching_flag="--no-watching"
    fi
    pure_python_flag=""
    if [ "${pure_python}" == "True" ]; then
        pure_python_flag="--pure-python"
    fi
    unsafe_building_flag=""
    if [ "${unsafe_building}" == "True" ]; then
        unsafe_building_flag="--unsafe-building"
    fi
    # Run utility and exit
    colorprint "Running the $(basename "${utility}") utility" "yellow"
    trap : 0
    if [ -z "${utility_args[0]}" ] && [ ${#utility_args[@]} == 1 ]; then
        called_from_concept="True" "${utility}"
    else
        called_from_concept="True" "${utility}" "${utility_args[@]}"
    fi
    # Print out the total execution time for this utility
    "${python}" -B -c "from commons import *
print('Total execution time: {}'.format(time_since(${start_time_epoch})))"
    exit 0
fi

# Sensible paths (for clean printout)
if [ "${main_as_command}" == "no" ]; then
    main_rel="$(sensible_path ${main})"
else
    main_rel="${main}"
fi
params_rel="$(sensible_path ${params})"
logs_dir_rel="$(sensible_path ${logs_dir})"

# Make a hidden copy of the parameter file in the params directory.
# This copy will be the parameter file actually used, freeing up the
# supplied parameter file for editing.
# Also make sure that this directory exists.
mkdir -p "${params_dir}"
params_cp="${params_dir}/.${start_time_human}"
if [ "${params}" == "${params_default}" ]; then
    params_cp_info=""
else
    cp "${params}" "${params_cp}"
    params_cp_info=" (copied to \"$(sensible_path ${params_cp})\")"
fi
# Insert command-line parameters
# at the bottom of the copied parameter file.
"${python}" -B -c "
lines = [
    '',
    '# The above is a copy of the CO𝘕CEPT parameter file \"${params}\".',
    '# Below we apply any additional command-line parameters.',
]
for statement in '${command_line_params}'.split(';'):
    lines.append(statement.strip())
lines.append('')
with open('${params_cp}', 'a') as f:
    f.write('\n'.join(lines))
"

# Create the logs dir if it does not exist
mkdir -p "${logs_dir}"

# Either stop doing further actions, submit job or run it locally
if [ "${remote}" == "True" ]; then
    # Running remotely.
    # The remote job name.
    jobname="CONCEPT:${params_rel}:CONCEPT"
    # Detect what resource manager is used
    resource_manager="$(get_resource_manager)"
    # Prepare job script header dependent on the resource manager.
    # If no resource manager is used, default to slurm.
    if [ "${resource_manager}" == "slurm" ] || [ -z "${resource_manager}" ]; then
        # Split the 'nprocs' variable up into the number of nodes
        # and the number of processes per node, if both are given.
        if [[ "${nprocs}" == *':'* ]]; then
            nnodes=$("${python}" -B -c "print('${nprocs}'[:'${nprocs}'.index(':')])")
            nprocs_per_node=$("${python}" -B -c "print('${nprocs}'[('${nprocs}'.index(':') + 1):])")
            ((nprocs = nnodes*nprocs_per_node))
        else
            nnodes=0  # Has to be 0, not 1
            nprocs_per_node=${nprocs}
        fi
        # Compute dedicated memory per process in megabytes
        mb_per_process=$("${python}" -B -c "print(int(${memory}/(2**20*${nprocs})))")
        # Construct Slurm header
        jobscript_header="$(${python} -B -c "
directive_prefix = '#SBATCH'
lines = []
lines.append(f'{directive_prefix} --job-name=${jobname}')
lines.append(f'{directive_prefix} --partition=${queue}')
if ${nnodes}:
    lines.append(f'{directive_prefix} --nodes=${nnodes}')
    lines.append(f'{directive_prefix} --ntasks-per-node=${nprocs_per_node}')
else:
    lines.append(f'{directive_prefix} --ntasks=${nprocs}')
if ${memory}:
    lines.append(f'{directive_prefix} --mem-per-cpu=${mb_per_process}M')
if '${walltime}' != '00:00:00':
    lines.append(f'{directive_prefix} --time=${walltime}')
lines.append(f'{directive_prefix} --output=/dev/null')
lines.append(f'{directive_prefix} --error=/dev/null')
for job_directive in '${job_directive}'.split(';'):
    job_directive = job_directive.strip()
    if not job_directive:
        continue
    if not job_directive.startswith(directive_prefix):
        job_directive = f'{directive_prefix} {job_directive}'
    lines.append(job_directive)
lines.append('')
lines.append('# Get the ID of the current job')
lines.append('jobid=\"\${SLURM_JOB_ID%%.*}\"')
print('\n'.join(lines))
        ")"
    elif [ "${resource_manager}" == "torque" ]; then
        # Split the 'nprocs' variable up into the number of nodes
        # and the number of processes per node.
        if [[ "${nprocs}" != *':'* ]]; then
            colorprint "Error: When using TORQUE or PBS you need to specify the number of nodes \
and the number of processes per node" "red"
            exit 1
        fi
        nnodes=$("${python}" -B -c "print('${nprocs}'[:'${nprocs}'.index(':')])")
        nprocs_per_node=$("${python}" -B -c "print('${nprocs}'[('${nprocs}'.index(':') + 1):])")
        ((nprocs = nnodes*nprocs_per_node))
        # Compute dedicated memory per process in megabytes
        mb_per_process=$("${python}" -B -c "print(int(${memory}/(2**20*${nprocs})))")
        # Construct TORQUE header
        jobscript_header="$(${python} -B -c "
directive_prefix = '#PBS'
lines = []
lines.append(f'{directive_prefix} -N ${jobname}')
lines.append(f'{directive_prefix} -q ${queue}')
lines.append(f'{directive_prefix} -l nodes=${nnodes}:ppn=${nprocs_per_node}')
if ${memory}:
    lines.append(f'{directive_prefix} -l pmem=${mb_per_process}mb')
if '${walltime}' != '00:00:00':
    lines.append(f'{directive_prefix} -l walltime=${walltime}')
lines.append(f'{directive_prefix} -o /dev/null')
lines.append(f'{directive_prefix} -e /dev/null')
for job_directive in '${job_directive}'.split(';'):
    job_directive = job_directive.strip()
    if not job_directive:
        continue
    if not job_directive.startswith(directive_prefix):
        job_directive = f'{directive_prefix} {job_directive}'
    lines.append(job_directive)
lines.append('')
lines.append('# Get the ID of the current job')
lines.append('jobid=\"\${PBS_JOBID%%.*}\"')
print('\n'.join(lines))
        ")"
    fi
    # Prepare text with the total memory consumption
    memory_in_tb=$("${python}" -B -c "print(int(${memory}/(2**40)))")
    memory_in_gb=$("${python}" -B -c "print(int(${memory}/(2**30)))")
    memory_in_mb=$("${python}" -B -c "print(int(${memory}/(2**20)))")
    if [ ${memory_in_tb} -gt 1 ]; then
        memory_display="${memory_in_tb} TB"
    elif [ ${memory_in_gb} -gt 1 ]; then
        memory_display="${memory_in_gb} GB"
    elif [ ${memory_in_mb} -gt 1 ]; then
        memory_display="${memory_in_mb} MB"
    else
        memory_display=""
    fi
    # The MPI executor command when run remotely.
    # When using Slurm, it is best to use the srun command rather than
    # calling mpiexec directly.
    # If the mpi_executor variable is already defined, we use this.
    if [ -z "${mpi_executor}" ]; then
        mpi_executor="${mpiexec}${mpiexec_args}"
        if [ "${resource_manager}" == "slurm" ]; then
            if get_command "srun" >/dev/null; then
                # Use the Slurm srun command as the MPI executor.
                # We further disable process binding/affinity in Slurm.
                mpi_executor="srun --cpu_bind=none"
                # When using OpenMPI with Slurm, it's often necessary to
                # additionally pass the --mpi=openmpi to srun. Here we
                # add this option if it is supported by srun.
                if [ "${mpi_implementation}" == "openmpi" ] \
                    && srun --mpi=list 2>&1 | grep "openmpi" >/dev/null; then
                    mpi_executor="${mpi_executor} --mpi=openmpi"
                fi
            else
                colorprint "Warning: Using Slurm but cannot find the srun command. \
Using mpiexec instead." "red"
            fi
        fi
    fi
    # Write job script file
    jobscript="${this_dir}/jobscript"
    printf "#!/usr/bin/env bash
${jobscript_header}

# Source the concept script
source '${concept}'

# MPI executor
mpi_executor='${mpi_executor}'

${mpi_env}

# Variables
logs_dir='${logs_dir}'
logs_dir_rel='${logs_dir_rel}'
main='${main}'
main_as_command='${main_as_command}'
main_rel='${main_rel}'
memory_display='${memory_display}'
nnodes=${nnodes}
nprocs=${nprocs}
params='${params}'
params_cp='${params_cp}'
params_cp_info='${params_cp_info}'
params_rel='${params_rel}'
pure_python='${pure_python}'

# Change to the logs directory,
# so that autogenerated files will be dumped there.
cd \"\${logs_dir}\"

# Print start messages
if [ \"\${pure_python}\" == \"True\" ]; then
    colorprint \"Running CO𝘕CEPT job \${jobid} in pure Python mode remotely on \$(hostname -f)\" \
               \"yellow\" > \"\${logs_dir}/\${jobid}\"
else
    colorprint \"Running CO𝘕CEPT job \${jobid} remotely on \$(hostname -f)\" \
               \"yellow\" > \"\${logs_dir}/\${jobid}\"
fi
echo \"Entry point:    \\\\\"\${main_rel}\\\\\"\" >> \"\${logs_dir}/\${jobid}\"
if [ \"\${params_rel}\" == \"None\" ]; then
    echo \"Parameter file: \${params_rel}\" >> \"\${logs_dir}/\${jobid}\"
else
    echo \"Parameter file: \\\\\"\${params_rel}\\\\\"\${params_cp_info}\" \
          >> \"\${logs_dir}/\${jobid}\"
fi
echo \"Log file:       \\\\\"\${logs_dir_rel}/\${jobid}\\\\\"\" >> \"\${logs_dir}/\${jobid}\"
if [ -n \"\${memory_display}\" ]; then
    echo \"Memory:         \${memory_display}\" >> \"\${logs_dir}/\${jobid}\"
fi

# Prepare Python options
if [ \"\${main_as_command}\" == \"yes\" ]; then
    # Run main as Python command
    main_as_library=\"\${main}\"
    m_flag=\"-c\"
else
    if [ \"\${pure_python}\" == \"True\" ]; then
        # Run main as normal Python script
        main_as_library=\"\${main}\"
        m_flag=\"\"
    else
        main_as_library=\"\${main%%.*}.so\"
        if [ -f \"\${main_as_library}\" ]; then
            # Run main as compiled library module
            main_as_library=\"\$(basename \"\${main_as_library}\")\"
            m_flag=\"-m\"
        else
            # Run main as normal Python script,
            # even though the CO𝘕CEPT modules are compiled.
            main_as_library=\"\${main}\"
            m_flag=\"\"
        fi
    fi
fi

# Run the code. Both stdout and stderr are being logged
# to logs_dir/jobid, while the stderr alone is also logged
# to logs_dir/jobid_err.
(                                                    \\
    cd \"\${concept_dir}\"                           \\
    && \${mpi_executor}                              \\
        \"\${python}\" -B                            \\
            \${m_flag} \"\${main_as_library}\"       \\
            \"params='\${params}'\"                  \\
            \"params_cp='\${params_cp}'\"            \\
            \"jobid='\${jobid}'\"                    \\
        >> \"\${logs_dir}/\${jobid}\"                \\
        2>> >(tee -a \"\${logs_dir}/\${jobid}_err\") \\
)

# Run complete. Do cleanup.
if [ -f \"\${logs_dir}/\${jobid}_err\" ] && [ ! -s \"\${logs_dir}/\${jobid}_err\" ]; then
    # Remove error log if empty
    rm \"\${logs_dir}/\${jobid}_err\"
else
    colorprint \"\\\nSome warnings/errors occured during CO𝘕CEPT run!\" \"red\" \
               >> \"\${logs_dir}/\${jobid}\" 2>&1
    colorprint \"Check the following error log for more information:\"  \"red\" \
               >> \"\${logs_dir}/\${jobid}\" 2>&1
    colorprint \"\\\\\"\${logs_dir}/\${jobid}_err\\\\\"\"               \"red\" \
               >> \"\${logs_dir}/\${jobid}\" 2>&1
fi
" > "${jobscript}"
    # Exit now if no resource manager was found
    if [ -z "${resource_manager}" ]; then
        echo "Could not find any resource manager. \
Is Slurm/TORQUE/PBS installed and on the PATH?"
        echo "An almost complete Slurm job script has been saved to \"${jobscript}\"."
        trap : 0
        exit 0
    fi
    # Only submit if a queue is specified
    if [ "${queue}" == "${queue_unspecified}" ]; then
        colorprint "Error: Cannot submit job as no queue is specified.\n\
An almost complete job script has been saved to \"${jobscript}\"." "red"
        exit 1
    fi
    # Submit the remote job from within the logs directory,
    # so that autogenerated files will be dumped there.
    colorprint "\nSubmitting job" "yellow"
    if [ "${resource_manager}" == 'slurm' ]; then
        jobid=$(cd "${logs_dir}" && sbatch "${jobscript}")
    elif [ "${resource_manager}" == 'torque' ]; then
        jobid=$(cd "${logs_dir}" && qsub "${jobscript}")
    fi
    jobid=$(echo "${jobid}" | awk '{print $NF}')
    jobid="${jobid%.*}"
    echo "Job ${jobid} (parameter file \"${params_rel}\") submitted to queue ${queue}"
    # Deactivate traps before exiting
    trap : 0
    # Invoke the watch utility on the submitted job
    # unless --no-watching was supplied.
    if [ "${no_watching}" == "False" ]; then
        # Invoke the watch utility on the submitted job
        echo "You can now kill (Ctrl-C) this script without cancelling the job"
        sleep 1
        "${concept}" -u watch "${jobid}"
    fi
    exit 0
else
    # Run locally
    if [[ "${nprocs}" == *':'* ]]; then
        nnodes=$("${python}" -B -c "print('${nprocs}'[:'${nprocs}'.index(':')])")
        if [ "${nnodes}" == "1" ]; then
            nprocs=$("${python}" -B -c "print('${nprocs}'[('${nprocs}'.index(':') + 1):])")
        else
            colorprint "You may not specify a number of nodes when running locally" "red"
            exit 1
        fi
    fi
    # Construct a jobid that does not conflict with the content
    # of the logs dir, using binary search.
    jobid=0
    if [ -f "${logs_dir}/${jobid}" ]; then
        ((jobid += 1))
    fi
    if [ -f "${logs_dir}/${jobid}" ]; then
        while [ -f "${logs_dir}/${jobid}" ]; do
            ((jobid *= 2))
        done
        jobid=$("${python}" -B -c "
import os
jobid_upper = ${jobid}
jobid_lower = ${jobid}//2
jobid = (jobid_upper + jobid_lower)//2
while jobid_upper - jobid_lower > 1:
    jobid = round(0.5*(jobid_upper + jobid_lower))
    if os.path.isfile(f'${logs_dir}/{jobid:.0f}'):
        jobid_lower = jobid
    else:
        jobid_upper = jobid
if os.path.isfile(f'${logs_dir}/{jobid:.0f}'):
    jobid += 1
print(f'{jobid:.0f}')
")
    fi
    # Print start message
    echo
    if [ "${pure_python}" == "True" ]; then
        colorprint "Running CO𝘕CEPT in pure Python mode" "yellow" | tee    "${logs_dir}/${jobid}"
    else
        colorprint "Running CO𝘕CEPT" "yellow"                     | tee    "${logs_dir}/${jobid}"
    fi
    echo "Entry point:    \"${main_rel}\""                        | tee -a "${logs_dir}/${jobid}"
    if [ "${params_rel}" == "None" ]; then
        echo "Parameter file: ${params_rel}"                      | tee -a "${logs_dir}/${jobid}"
    else
        echo "Parameter file: \"${params_rel}\"${params_cp_info}" | tee -a "${logs_dir}/${jobid}"
    fi
    echo "Log file:       \"${logs_dir_rel}/${jobid}\""           | tee -a "${logs_dir}/${jobid}"
    # Prepare Python options
    if [ "${main_as_command}" == "yes" ]; then
        # Run main as Python command
        main_as_library="${main}"
        m_flag="-c"
    else
        if [ "${pure_python}" == "True" ]; then
            # Run main as normal Python script
            main_as_library="${main}"
            m_flag=""
        else
            main_as_library="${main%.*}.so"
            if [ -f "${main_as_library}" ]; then
                # Run main as compiled library module
                main_as_library="$(basename "${main_as_library}")"
                m_flag="-m"
            else
                # Run main as normal Python script,
                # even though the CO𝘕CEPT modules are compiled.
                main_as_library="${main}"
                m_flag=""
            fi
        fi
    fi
    i_flag=""
    if [ "${interactive}" == "True" ]; then
        i_flag="-i"
    fi
    # Run the code. Print stdout and stderr to the terminal while at the
    # same time logging them to logs_dir/jobid. The stderr alone is also
    # logged to logs_dir/jobid_err.
    # We want the exit code of mpiexec, but it is not easily retrieved
    # due to the logging. We therefore print the exit code to a
    # temporary file which we then read and discard.
    exit_code_filename="${this_dir}/.exit_code_${start_time_human}"
    (("${mpiexec}" -n "${nprocs}"                             \
                   ${mpiexec_args}                            \
                   "${python}" -B ${i_flag}                   \
                               ${m_flag} "${main_as_library}" \
                               "params='${params}'"           \
                               "params_cp='${params_cp}'"     \
                               "jobid='${jobid}'"             \
     | tee -a "${logs_dir}/${jobid}";                         \
       echo "${PIPESTATUS[0]}" > "${exit_code_filename}"      \
     ) 3>&1 1>&2 2>&3 | tee -a "${logs_dir}/${jobid}"         \
                               "${logs_dir}/${jobid}_err") 3>&1 1>&2 2>&3
    # Get the exit code from the temporary file
    sleep_time=0.01
    sleep_max=1000
    slept=0
    while [ ! -f "${exit_code_filename}" ]; do
        sleep ${sleep_time}
        ((slept += 1))
        if [ ${slept} -ge ${sleep_max} ]; then
            break
        fi
    done
    exit_code=1
    if [ -f "${exit_code_filename}" ]; then
        exit_code="$(cat "${exit_code_filename}")"
        if [ -z "${exit_code}" ]; then
            exit_code=1
        fi
    fi
    # Run complete. Do cleanup.
    rm -f "${exit_code_filename}"
    if [ -f "${logs_dir}/${jobid}_err" ]; then
        if (   [ ! -s "${logs_dir}/${jobid}_err" ] \
            || ! grep -v '>>>' "${logs_dir}/${jobid}_err" >/dev/null 2>&1); then
            # Remove error log if empty
            rm "${logs_dir}/${jobid}_err"
        else
            colorprint "\nSome warnings/errors occured during CO𝘕CEPT run!\n\
Check the following error log for more information:\n\
\"${logs_dir}/${jobid}_err\"" "red" 2>&1 | tee -a "${logs_dir}/${jobid}" 1>&2
        fi
    fi
    # If the CO𝘕CEPT run exited erroneously, exit now
    if [ "${exit_code}" != "0" ]; then
        # Note that the real exit will be handled by the abort function
        exit ${exit_code}
    fi
    # Deactivate traps and exit
    trap : 0
    exit 0
fi
